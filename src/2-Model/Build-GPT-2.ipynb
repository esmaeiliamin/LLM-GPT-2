{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#FFE15D'><b>ðŸ’Ž Build GPT-2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time(model, x, num_runs=10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        model(*x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens from pytorch file\n",
    "train_token_ids = torch.load('tokenized-train-samples_vocab-10k.pt')\n",
    "valid_token_ids = torch.load('tokenized-valid-samples_vocab-10k.pt')\n",
    "\n",
    "print(\"ðŸ“Š Number of Tokens\")\n",
    "print(f\"ðŸ”¹ Train: {len(train_token_ids):,} tokens\")\n",
    "print(f\"ðŸ”¹ Valid: {len(valid_token_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = prepare_data(data, seq_len+1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample[:-1], sample[1:]#.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128\n",
    "\n",
    "train_set = TinyStoriesDataset(train_token_ids, seq_len)\n",
    "valid_set = TinyStoriesDataset(valid_token_ids, seq_len)\n",
    "\n",
    "print(f\"ðŸ“Š Number of Samples\")\n",
    "print(f\"ðŸ”¹ Train: {len(train_set):,} samples\")\n",
    "print(f\"ðŸ”¹ Valid: {len(valid_set):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)#, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)#, num_workers=4)\n",
    "\n",
    "print(f\"ðŸ“Š Number of Batches\")\n",
    "print(f\"ðŸ”¹ Train: {len(train_loader):,} batches\")\n",
    "print(f\"ðŸ”¹ Valid: {len(valid_loader):,} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"ðŸ“Š Batch Shapes\")\n",
    "print(f\"ðŸ”¹ Input: {x_batch.shape}\")\n",
    "print(f\"ðŸ”¹ Target: {y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = nn.Embedding(tokenizer.get_vocab_size(), 100)\n",
    "wte(torch.tensor([1, 2, 100])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpe = nn.Embedding(seq_len, 100)\n",
    "wpe(torch.tensor([1, 2, 100])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = wte(x_batch) + wpe(torch.arange(x_batch.shape[1]))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = k = v = x\n",
    "print(q.shape)\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "scores = q @ k.transpose(-2, -1) / (k.shape[-1]**0.5)\n",
    "scores.masked_fill_(mask ==0, float(-torch.inf))\n",
    "scores = scores.softmax(dim=-1)\n",
    "print(scores.shape)\n",
    "\n",
    "z = scores @ v\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "    mask = torch.tril(torch.ones(q.shape[-2], q.shape[-2])).to(device)\n",
    "    scores = q @ k.transpose(-2, -1) / (k.shape[-1]**0.5)\n",
    "    scores.masked_fill_(mask==0, float(-torch.inf))\n",
    "    scores = scores.softmax(dim=-1)\n",
    "    z = scores @ v\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dot_product_attention(x.to(device), x.to(device), x.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn((128, 1024, 768), device=device)\n",
    "k = torch.randn((128, 1024, 768), device=device)\n",
    "v = torch.randn((128, 1024, 768), device=device)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dot_product_attention(q, k, v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(scaled_dot_product_attention, (q, k, v), num_runs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.scaled_dot_product_attention(q, k, v, is_causal=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(scaled_dot_product_attention(q, k, v) - F.scaled_dot_product_attention(q, k, v, is_causal=True)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(F.scaled_dot_product_attention, (q, k, v), num_runs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    n_embd: int = 100\n",
    "    n_head: int = 5\n",
    "\n",
    "config = GPTConfig()\n",
    "config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
    "\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)\n",
    "mha(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.arange(24).view(2, 2, 3, 2)\n",
    "print(xx)\n",
    "xx.reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(mha.to(device), (x.to(device),), num_runs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Feed Forward (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    n_embd: int = 100\n",
    "    n_head: int = 5\n",
    "    f_expnd: float = 4\n",
    "\n",
    "config = GPTConfig()\n",
    "config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.f_expnd = config.f_expnd\n",
    "\n",
    "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
    "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
    "        self.down_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.up_proj(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = FeedForward(config)\n",
    "mlp(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params(mlp)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(mlp, (x, ), num_runs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderBlock(config)\n",
    "decoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params(decoder) * 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(decoder, (x, ), num_runs=20) * 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    vocab_size: int = 10_000\n",
    "    seq_len: int = 128\n",
    "    n_layer: int = 12\n",
    "    n_embd: int = 100\n",
    "    n_head: int = 5\n",
    "    f_expnd: float = 4\n",
    "\n",
    "\n",
    "config = GPTConfig()\n",
    "config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.seq_len, config.n_embd)\n",
    "        # self.decoders = nn.Sequential(*[DecoderBlock(config) for _ in range(config.n_layer)])\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)])\n",
    "        self.lnf = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        # self.lm_head.weight.data.uniform_(-1/self.lm_head.in_features**0.5, 1/self.lm_head.in_features**0.5)\n",
    "        # nn.init.uniform_(self.lm_head.weight, -1/self.lm_head.in_features**0.5, 1/self.lm_head.in_features**0.5)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'residual'):\n",
    "                std *= (2*self.config.n_layer)**-0.5\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
    "\n",
    "        # x = self.decoders(x)\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x)\n",
    "\n",
    "        x = self.lnf(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(config).to(device)\n",
    "model(x_batch.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params(model), num_trainable_params(model.decoders), num_trainable_params(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_time(model, (x_batch.to(device),), num_runs=100) * 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    GPTConfig(\n",
    "        seq_len=256, vocab_size=10_000, n_layer=4, n_embd=256, n_head=4\n",
    "        )).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.decoders[0].mha.c_proj.weight.flatten().detach().cpu(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.wpe.weight.flatten()[:100_000].detach().cpu(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.decoders[2].mlp.down_proj.weight.flatten().detach().cpu(), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257 # number of tokens\n",
    "    seq_len: int = 1024 # max sequence length\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    f_expnd: int = 4 # expansion factor in mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig()).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params(model)\n",
    "\n",
    "print(f\"ðŸ”¹ Trainable parameters: {num_trainable_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    GPTConfig(\n",
    "        seq_len=256, vocab_size=10_000, n_layer=4, n_embd=256, n_head=4\n",
    "        )).to(device)\n",
    "\n",
    "print(f\"ðŸ”¹ Trainable parameters: {num_trainable_params(model):,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layers\n",
    "wte_params = num_trainable_params(model.wte)  # Word Token Embedding\n",
    "wpe_params = num_trainable_params(model.wpe)  # Position Embedding\n",
    "\n",
    "# Optional: Classifier head\n",
    "lm_head_params = num_trainable_params(model.lm_head)\n",
    "\n",
    "# Total trainable parameters without weight tying\n",
    "total_params_without_tying = num_trainable_params(model)\n",
    "\n",
    "# Total trainable parameters with weight tying\n",
    "total_params_with_tying = total_params_without_tying - wte_params\n",
    "\n",
    "# Core model params (excluding embeddings and head)\n",
    "core_params = total_params_without_tying - wte_params - wpe_params - lm_head_params\n",
    "\n",
    "# Print results\n",
    "print(f\"ðŸ”¹ Total trainable parameters without weight tying: {total_params_without_tying:,}\")\n",
    "print(f\"ðŸ”¹ Total trainable parameters with weight tying: {total_params_with_tying:,}\")\n",
    "print(f\"ðŸ”¹ Embedding parameters (WTE + WPE): {wte_params + wpe_params:,}\")\n",
    "print(f\"ðŸ”¹  â””â”€ WTE: {wte_params:,}\")\n",
    "print(f\"ðŸ”¹  â””â”€ WPE: {wpe_params:,}\")\n",
    "print(f\"ðŸ”¹ Classifier head parameters: {lm_head_params:,}\")\n",
    "print(f\"ðŸ”¹ Transformer core (excluding embeddings & head): {core_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: x_batch -> model\n",
    "logits = model(x_batch.to(device))\n",
    "print(logits.shape)\n",
    "print(logits[0, 0])\n",
    "logits[0, 0].softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: (logits, y_batch) -> loss (cross entropy)\n",
    "\n",
    "print(logits.view(-1, logits.shape[-1]).shape)\n",
    "print(y_batch.flatten().shape, y_batch.dtype)\n",
    "\n",
    "loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y_batch.flatten().to(device).long())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: backward\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "model.decoders[0].mha.qkv_proj.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "model.decoders[0].mha.qkv_proj.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x_batch.to(device))\n",
    "loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y_batch.flatten().to(device).long())\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for iter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    logits = model(x_batch.to(device))\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y_batch.flatten().to(device).long())\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if iter > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.log(torch.tensor(0.5)) # binary classification\n",
    "-torch.log(torch.tensor(1/10)) # 10 class\n",
    "-torch.log(torch.tensor(1/10_000)) # 10_000 class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    GPTConfig(\n",
    "        seq_len=256, vocab_size=10_000, n_layer=4, n_embd=256, n_head=4\n",
    "        )).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x_batch.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = logits[0, 0]\n",
    "print(output.min(), output.max())\n",
    "\n",
    "scores = output.softmax(dim=0)\n",
    "print(scores.min(), scores.max()) # 1/10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(output.flatten()[:100_000].detach().cpu(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.lm_head.weight.flatten()[:100_000].detach().cpu(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(model.lm_head.weight.flatten()[:100_000].detach().cpu(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = logits[0, 0]\n",
    "print(output.min(), output.max())\n",
    "\n",
    "scores = output.softmax(dim=0)\n",
    "print(scores.min(), scores.max()) # 1/10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F.cross_entropy(output, y_batch[0, 0].to(device).long()))\n",
    "F.cross_entropy(logits.view(-1, logits.shape[-1]), y_batch.flatten().to(device).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "with torch.inference_mode():\n",
    "    for iter, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        logits = model(x_batch.to(device))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y_batch.flatten().to(device).long())\n",
    "        print(loss.item())\n",
    "        if iter > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for iter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "    logits = model(x_batch.to(device))\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y_batch.flatten().to(device).long())\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if iter > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "\n",
    "prompt = 'In last night'\n",
    "tokens = tokenizer.encode(prompt).ids\n",
    "tokens = torch.tensor(tokens, dtype=torch.int, device=device).unsqueeze(0)\n",
    "print(tokens)\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for i in range(max_seq_len):\n",
    "        logits = model(tokens)\n",
    "        scores = logits[0, [-1]].softmax(dim=-1)\n",
    "        idx = scores.argmax(keepdims=True)\n",
    "        tokens = torch.cat((tokens, idx), dim=-1)\n",
    "\n",
    "tokenizer.decode(tokens[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
