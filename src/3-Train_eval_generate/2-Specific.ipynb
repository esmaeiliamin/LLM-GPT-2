{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#FFE15D'><b>üíé Train, Evaluate, and Generate Functions (LLM-specific) </b></font><font color='#FF0B55'><b>[Walkthrough]</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from itertools import cycle\n",
    "from termcolor import colored\n",
    "from dataclasses import dataclass\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmetrics import MeanMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking function\n",
    "def calculate_time(model, x, num_runs=10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128 # Desired sequence length for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens from pytorch file\n",
    "train_token_ids = torch.load('tokenized-train-samples_vocab-10k.pt')\n",
    "valid_token_ids = torch.load('tokenized-valid-samples_vocab-10k.pt')\n",
    "\n",
    "print(\"üìä Number of Tokens\")\n",
    "print(f\"üîπ Train: {len(train_token_ids):,} tokens\")\n",
    "print(f\"üîπ Valid: {len(valid_token_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = prepare_data(data, seq_len+1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample.long()#[:-1], sample[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # QKV linear\n",
    "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
    "        # Scaled Dot Product Attention using pytorch\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # Reshape and final projection\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Feed Forward (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.f_expnd = config.f_expnd\n",
    "\n",
    "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
    "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
    "        self.down_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.up_proj(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        # Multi Head Attention\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        # Feed Forward Neural Network\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # Token embedding\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.n_embd) # Position embedding\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]) # Decoders\n",
    "        self.lnf = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Classifier\n",
    "        self.lm_head.weight = self.wte.weight # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'residual'):\n",
    "                std *= (2*self.config.n_layer)**-0.5\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        # Token Embedding + Position Embedding\n",
    "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
    "        # Decoders\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x)\n",
    "        # Classifier\n",
    "        x = self.lnf(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257 # number of tokens\n",
    "    max_seq_len: int = 1024 # max sequence length\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    f_expnd: int = 4 # expansion factor in mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Functions ‚öôÔ∏è**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "seq_len = 128\n",
    "train_set = TinyStoriesDataset(train_token_ids, seq_len)\n",
    "valid_set = TinyStoriesDataset(valid_token_ids, seq_len)\n",
    "\n",
    "batch_size = 192\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)#, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)#, num_workers=4)\n",
    "\n",
    "print(f\"üìä Number of Batches\")\n",
    "print(f\"üîπ Train: {len(train_loader):,} batches\")\n",
    "print(f\"üîπ Valid: {len(valid_loader):,} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    GPTConfig(\n",
    "        max_seq_len=256,\n",
    "        vocab_size=10_000,\n",
    "        n_embd=128,\n",
    "        n_layer=8,\n",
    "        n_head=16\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "print(model)\n",
    "num_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 6e-4\n",
    "weight_decay = 0.1\n",
    "use_fused = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=weight_decay,\n",
    "    fused=use_fused\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger class for saving and plotting training logs\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    Manages training history logging, saving to disk, and plotting learning curves.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir='logs', run_name='default_run'):\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.run_name = run_name\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'valid_loss': [],\n",
    "            'best_loss_valid': float('inf'),\n",
    "            'seen_tokens': []\n",
    "        }\n",
    "\n",
    "    def log(self, train_loss, valid_loss, seen_tokens):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['valid_loss'].append(valid_loss)\n",
    "        self.history['seen_tokens'].append(seen_tokens)\n",
    "\n",
    "    def save(self):\n",
    "        # Save history\n",
    "        file_path = os.path.join(self.log_dir, f'{self.run_name}.json')\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "        # Save best model and optimizer\n",
    "        current_loss_valid = self.history['valid_loss'][-1]\n",
    "        if current_loss_valid < self.history['best_loss_valid']:\n",
    "            log = dict(model=model.state_dict(), optimizer=optimizer)\n",
    "            torch.save(log, f'{self.log_dir}/best-model.pt')\n",
    "            self.history['best_loss_valid'] = current_loss_valid\n",
    "            print(\"‚úÖ Model Saved!\")\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.history['seen_tokens'], self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['seen_tokens'], self.history['valid_loss'], label='Valid Loss')\n",
    "        plt.xlabel('Seen Tokens')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training Curve: {self.run_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.log_dir, f'{self.run_name}_curve.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Train ‚û∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class to manage model training, evaluation and reporting\n",
    "class LLMTrainer:\n",
    "    \"\"\"\n",
    "    Trainer handles training loops, periodic evaluation, logging, and sample generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optimizer, train_loader, valid_loader,\n",
    "                 loss_fn=F.cross_entropy, device='cuda',\n",
    "                 total_tokens=10_000_000, log_interval_tokens=1_000_000,\n",
    "                 generation=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.seen_tokens = 0\n",
    "        self.total_tokens = total_tokens\n",
    "        self.token_eval_counter = 0\n",
    "        self.log_interval_tokens = log_interval_tokens\n",
    "\n",
    "        self.logger = Logger(log_dir='logs', run_name='gpt2_tinystories')\n",
    "        self._print_config_summary()\n",
    "\n",
    "        self.generation = generation\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop that stops when total token count is reached.\n",
    "        \"\"\"\n",
    "        # # Initial evaluation before any training\n",
    "        # initial_loss = self.evaluate()\n",
    "        # self.logger.log(initial_loss, initial_loss, 0)\n",
    "        # print(f\"üë∂ [Initial] Train Loss (Untrained Model): {initial_loss:.4f}\\n\")\n",
    "\n",
    "        loss_train = MeanMetric()\n",
    "        self.model.train()\n",
    "        train_iter = cycle(self.train_loader)\n",
    "\n",
    "        batches = 0\n",
    "        total_time_elapsed = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        with tqdm(total=self.total_tokens, desc=\"Training\", unit=\"t\") as pbar:\n",
    "            while self.seen_tokens < self.total_tokens:\n",
    "                # Get inputs\n",
    "                inputs = next(train_iter).to(self.device)\n",
    "                # Forward pass\n",
    "                logits = self.model(inputs[:, :-1])\n",
    "                # Calculate loss\n",
    "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                # Clip gradients\n",
    "                nn.utils.clip_grad.clip_grad_norm_(self.model.parameters(), max_norm=1.)\n",
    "                # Update model\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                # Calc running loss\n",
    "                loss_train.update(loss.item(), inputs.shape[0])\n",
    "\n",
    "                num_tokens_this_batch = inputs[:, :-1].numel()\n",
    "                self.seen_tokens += num_tokens_this_batch\n",
    "                self.token_eval_counter += num_tokens_this_batch\n",
    "                batches += 1\n",
    "                elapsed = time.time() - start_time\n",
    "                batches_per_sec = batches / elapsed\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    \"B/S\": f\"{batches_per_sec:.2f}\",\n",
    "                    \"Loss\": f\"{loss_train.compute().item():.4f}\",\n",
    "                    \"LR\": f\"{self.optimizer.param_groups[0]['lr']:.2e}\",\n",
    "                })\n",
    "                pbar.update(num_tokens_this_batch)\n",
    "\n",
    "                if (self.token_eval_counter >= self.log_interval_tokens) or (self.seen_tokens >= self.total_tokens):\n",
    "                    # Evaluate\n",
    "                    loss_valid = self.evaluate()\n",
    "                    print(f\"\\nValid Loss: {loss_valid:.4f}\")\n",
    "                    # Log\n",
    "                    self.logger.log(loss_train.compute().item(), loss_valid, self.seen_tokens)\n",
    "                    self.logger.save()\n",
    "                    # Generate\n",
    "                    if self.generation:\n",
    "                        self.generate()\n",
    "                    # Reset\n",
    "                    self.token_eval_counter = 0\n",
    "                    batches = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "        self.logger.plot()\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model on validation set.\n",
    "        \"\"\"\n",
    "        loss_valid = MeanMetric()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in self.valid_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                logits = self.model(inputs[:, :-1])\n",
    "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "                loss_valid.update(loss.item(), inputs.shape[0])\n",
    "        return loss_valid.compute().item()\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate and print text samples from the model.\n",
    "        \"\"\"\n",
    "        generated_texts = []\n",
    "        for prompt in self.generation.prompts:\n",
    "            gen_text = generate(\n",
    "                self.model, self.generation.tokenizer, prompt,\n",
    "                n_rep=self.generation.n_rep,\n",
    "                max_seq_len=self.generation.max_seq_len,\n",
    "                T=self.generation.T, top_k=self.generation.top_k,\n",
    "                seed=self.generation.seed)\n",
    "            generated_texts.append(gen_text)\n",
    "        # TODO: Save\n",
    "        # Print\n",
    "        # print(150*'.')\n",
    "        # item = 0\n",
    "        # prompt0 = self.generation.prompts[item]\n",
    "        # for gen_text in generated_texts[item]:\n",
    "        #     print(colored(f\"\\n{prompt0}\", \"green\"), end=' ')\n",
    "        #     print(colored(f\"{gen_text[len(prompt0):]}\", \"cyan\"))\n",
    "        #     print(150*'.')\n",
    "        # print()\n",
    "        item = 0\n",
    "        prompt0 = self.generation.prompts[item]\n",
    "        gen_text0 = generated_texts[item][0]\n",
    "        print(colored(f\"\\n{prompt0}\", \"green\"), end=' ')\n",
    "        print(colored(f\"{gen_text0[len(prompt0):]}\", \"cyan\"))\n",
    "        print()\n",
    "\n",
    "    def _print_config_summary(self):\n",
    "        \"\"\"\n",
    "        Print a summary table of training configuration.\n",
    "        \"\"\"\n",
    "        table = PrettyTable()\n",
    "        table.title = \"Training Configuration Summary\"\n",
    "        table.field_names = [\"Component\", \"Details\"]\n",
    "        # Model\n",
    "        table.add_row([\"Model Type\", str(self.model.config).replace(\"Config\", \"\")])\n",
    "        # Optimizer\n",
    "        optimizer_name = self.optimizer.__class__.__name__\n",
    "        optimizer_params = ', '.join([f\"{k}={v}\" for k, v in self.optimizer.defaults.items() if k in [\"lr\", \"betas\", \"weight_decay\", \"fused\"]])\n",
    "        optimizer_display = f\"{optimizer_name}({optimizer_params})\"\n",
    "        table.add_row([\"Optimizer\", optimizer_display])\n",
    "        # Parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        te_params = self.model.wte.weight.numel()\n",
    "        table.add_row([\"Total Parameters (Tr+TE)\", f\"{total_params:,} ({total_params-te_params:,}+{te_params:,})\"])\n",
    "\n",
    "        table.add_row([\"Loss Function\", self.loss_fn.__name__ if hasattr(self.loss_fn, '__name__') else str(self.loss_fn)])\n",
    "        table.add_row([\"Batch Shape\", f\"{self.train_loader.batch_size}x{self.train_loader.dataset[0].shape[-1]-1}\"])\n",
    "        table.add_row([\"Device\", self.device])\n",
    "        table.add_row([\"Max Tokens\", f\"{self.total_tokens:,}\"])\n",
    "        table.add_row([\"Log Interval Tokens\", f\"{self.log_interval_tokens:,}\"])\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    tokenizer: Tokenizer\n",
    "    prompts: list[str]\n",
    "    T: float = 0.9\n",
    "    max_seq_len: int = 128\n",
    "    top_k: int = 10\n",
    "    n_rep: int = 3\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model, optimizer, train_loader, valid_loader,\n",
    "    total_tokens=100_000, log_interval_tokens=50_000,\n",
    "    generation=GenerationConfig(tokenizer=tokenizer, prompts=[\"In last\", \"One day\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "loader = DataLoader(TensorDataset(torch.arange(4)), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "iterr = cycle(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=100, desc=\"Training\", unit='t') as pbar:\n",
    "    for i in range(10):\n",
    "        time.sleep(0.1)\n",
    "        pbar.set_postfix({'loss': torch.randn(1).item()})\n",
    "        pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Create a table\n",
    "table = PrettyTable()\n",
    "\n",
    "# Add columns\n",
    "table.field_names = [\"Name\", \"Position\", \"Age\"]\n",
    "\n",
    "# Add rows\n",
    "table.add_row([\"Alice\", \"Manager\", 35])\n",
    "table.add_row([\"Bob\", \"Data Analyst\", 29])\n",
    "table.add_row([\"Charlie\", \"Engineer\", 32])\n",
    "\n",
    "# Print the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
