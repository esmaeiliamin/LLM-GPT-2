{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#FFE15D'><b>üíé Train, Evaluate, and Generate Functions (LLM-specific) </b></font><font color='#FF0B55'><b>[Walkthrough]</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from itertools import cycle\n",
    "from termcolor import colored\n",
    "from dataclasses import dataclass\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmetrics import MeanMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking function\n",
    "def calculate_time(model, x, num_runs=10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128 # Desired sequence length for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens from pytorch file\n",
    "train_token_ids = torch.load('tokenized-train-samples_vocab-10k.pt')\n",
    "valid_token_ids = torch.load('tokenized-valid-samples_vocab-10k.pt')\n",
    "\n",
    "print(\"üìä Number of Tokens\")\n",
    "print(f\"üîπ Train: {len(train_token_ids):,} tokens\")\n",
    "print(f\"üîπ Valid: {len(valid_token_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = prepare_data(data, seq_len+1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample.long()#[:-1], sample[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # QKV linear\n",
    "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
    "        # Scaled Dot Product Attention using pytorch\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # Reshape and final projection\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Feed Forward (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.f_expnd = config.f_expnd\n",
    "\n",
    "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
    "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
    "        self.down_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.up_proj(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        # Multi Head Attention\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        # Feed Forward Neural Network\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # Token embedding\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.n_embd) # Position embedding\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]) # Decoders\n",
    "        self.lnf = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Classifier\n",
    "        self.lm_head.weight = self.wte.weight # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'residual'):\n",
    "                std *= (2*self.config.n_layer)**-0.5\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        # Token Embedding + Position Embedding\n",
    "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
    "        # Decoders\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x)\n",
    "        # Classifier\n",
    "        x = self.lnf(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257 # number of tokens\n",
    "    max_seq_len: int = 1024 # max sequence length\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    f_expnd: int = 4 # expansion factor in mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Functions ‚öôÔ∏è**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "seq_len = 128\n",
    "train_set = TinyStoriesDataset(train_token_ids, seq_len)\n",
    "valid_set = TinyStoriesDataset(valid_token_ids, seq_len)\n",
    "\n",
    "batch_size = 192\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)#, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)#, num_workers=4)\n",
    "\n",
    "print(f\"üìä Number of Batches\")\n",
    "print(f\"üîπ Train: {len(train_loader):,} batches\")\n",
    "print(f\"üîπ Valid: {len(valid_loader):,} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    GPTConfig(\n",
    "        max_seq_len=256,\n",
    "        vocab_size=10_000,\n",
    "        n_embd=128,\n",
    "        n_layer=8,\n",
    "        n_head=16\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "print(model)\n",
    "num_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 6e-4\n",
    "weight_decay = 0.1\n",
    "use_fused = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=weight_decay,\n",
    "    fused=use_fused\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger class for saving and plotting training logs\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    Manages training history logging, saving to disk, and plotting learning curves.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir='logs', run_name='default_run'):\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.run_name = run_name\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'valid_loss': [],\n",
    "            'best_loss_valid': float('inf'),\n",
    "            'seen_tokens': []\n",
    "        }\n",
    "\n",
    "    def log(self, train_loss, valid_loss, seen_tokens):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['valid_loss'].append(valid_loss)\n",
    "        self.history['seen_tokens'].append(seen_tokens)\n",
    "\n",
    "    def save(self):\n",
    "        # Save history\n",
    "        file_path = os.path.join(self.log_dir, f'{self.run_name}.json')\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=4)\n",
    "        # Save best model and optimizer\n",
    "        current_loss_valid = self.history['valid_loss'][-1]\n",
    "        if current_loss_valid < self.history['best_loss_valid']:\n",
    "            log = dict(model=model.state_dict(), optimizer=optimizer)\n",
    "            torch.save(log, f'{self.log_dir}/best-model.pt')\n",
    "            self.history['best_loss_valid'] = current_loss_valid\n",
    "            print(\"‚úÖ Model Saved!\")\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.history['seen_tokens'], self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['seen_tokens'], self.history['valid_loss'], label='Valid Loss')\n",
    "        plt.xlabel('Seen Tokens')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training Curve: {self.run_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.log_dir, f'{self.run_name}_curve.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Train ‚û∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class to manage model training, evaluation and reporting\n",
    "class LLMTrainer:\n",
    "    \"\"\"\n",
    "    Trainer handles training loops, periodic evaluation, logging, and sample generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optimizer, train_loader, valid_loader,\n",
    "                 loss_fn=F.cross_entropy, device='cuda',\n",
    "                 total_tokens=10_000_000, log_interval_tokens=1_000_000,\n",
    "                 generation=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "        self.seen_tokens = 0\n",
    "        self.total_tokens = total_tokens\n",
    "        self.token_eval_counter = 0\n",
    "        self.log_interval_tokens = log_interval_tokens\n",
    "\n",
    "        self.logger = Logger(log_dir='logs', run_name='gpt2_tinystories')\n",
    "        self._print_config_summary()\n",
    "\n",
    "        self.generation = generation\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Main training loop that stops when total token count is reached.\n",
    "        \"\"\"\n",
    "        # # Initial evaluation before any training\n",
    "        # initial_loss = self.evaluate()\n",
    "        # self.logger.log(initial_loss, initial_loss, 0)\n",
    "        # print(f\"üë∂ [Initial] Train Loss (Untrained Model): {initial_loss:.4f}\\n\")\n",
    "\n",
    "        loss_train = MeanMetric()\n",
    "        self.model.train()\n",
    "        train_iter = cycle(self.train_loader)\n",
    "\n",
    "        batches = 0\n",
    "        total_time_elapsed = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        with tqdm(total=self.total_tokens, desc=\"Training\", unit=\"t\") as pbar:\n",
    "            while self.seen_tokens < self.total_tokens:\n",
    "                # Get inputs\n",
    "                inputs = next(train_iter).to(self.device)\n",
    "                # Forward pass\n",
    "                logits = self.model(inputs[:, :-1])\n",
    "                # Calculate loss\n",
    "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                # Clip gradients\n",
    "                nn.utils.clip_grad.clip_grad_norm_(self.model.parameters(), max_norm=1.)\n",
    "                # Update model\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                # Calc running loss\n",
    "                loss_train.update(loss.item(), inputs.shape[0])\n",
    "\n",
    "                num_tokens_this_batch = inputs[:, :-1].numel()\n",
    "                self.seen_tokens += num_tokens_this_batch\n",
    "                self.token_eval_counter += num_tokens_this_batch\n",
    "                batches += 1\n",
    "                elapsed = time.time() - start_time\n",
    "                batches_per_sec = batches / elapsed\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    \"B/S\": f\"{batches_per_sec:.2f}\",\n",
    "                    \"Loss\": f\"{loss_train.compute().item():.4f}\",\n",
    "                    \"LR\": f\"{self.optimizer.param_groups[0]['lr']:.2e}\",\n",
    "                })\n",
    "                pbar.update(num_tokens_this_batch)\n",
    "\n",
    "                if (self.token_eval_counter >= self.log_interval_tokens) or (self.seen_tokens >= self.total_tokens):\n",
    "                    # Evaluate\n",
    "                    loss_valid = self.evaluate()\n",
    "                    print(f\"\\nValid Loss: {loss_valid:.4f}\")\n",
    "                    # Log\n",
    "                    self.logger.log(loss_train.compute().item(), loss_valid, self.seen_tokens)\n",
    "                    self.logger.save()\n",
    "                    # Generate\n",
    "                    if self.generation:\n",
    "                        self.generate()\n",
    "                    # Reset\n",
    "                    self.token_eval_counter = 0\n",
    "                    batches = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "        self.logger.plot()\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model on validation set.\n",
    "        \"\"\"\n",
    "        loss_valid = MeanMetric()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in self.valid_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                logits = self.model(inputs[:, :-1])\n",
    "                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "                loss_valid.update(loss.item(), inputs.shape[0])\n",
    "        return loss_valid.compute().item()\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate and print text samples from the model.\n",
    "        \"\"\"\n",
    "        generated_texts = []\n",
    "        for prompt in self.generation.prompts:\n",
    "            gen_text = generate(\n",
    "                self.model, self.generation.tokenizer, prompt,\n",
    "                n_rep=self.generation.n_rep,\n",
    "                max_seq_len=self.generation.max_seq_len,\n",
    "                T=self.generation.T, top_k=self.generation.top_k,\n",
    "                seed=self.generation.seed)\n",
    "            generated_texts.append(gen_text)\n",
    "        # TODO: Save\n",
    "        # Print\n",
    "        # print(150*'.')\n",
    "        # item = 0\n",
    "        # prompt0 = self.generation.prompts[item]\n",
    "        # for gen_text in generated_texts[item]:\n",
    "        #     print(colored(f\"\\n{prompt0}\", \"green\"), end=' ')\n",
    "        #     print(colored(f\"{gen_text[len(prompt0):]}\", \"cyan\"))\n",
    "        #     print(150*'.')\n",
    "        # print()\n",
    "        item = 0\n",
    "        prompt0 = self.generation.prompts[item]\n",
    "        gen_text0 = generated_texts[item][0]\n",
    "        print(colored(f\"\\n{prompt0}\", \"green\"), end=' ')\n",
    "        print(colored(f\"{gen_text0[len(prompt0):]}\", \"cyan\"))\n",
    "        print()\n",
    "\n",
    "    def _print_config_summary(self):\n",
    "        \"\"\"\n",
    "        Print a summary table of training configuration.\n",
    "        \"\"\"\n",
    "        table = PrettyTable()\n",
    "        table.title = \"Training Configuration Summary\"\n",
    "        table.field_names = [\"Component\", \"Details\"]\n",
    "        # Model\n",
    "        table.add_row([\"Model Type\", str(self.model.config).replace(\"Config\", \"\")])\n",
    "        # Optimizer\n",
    "        optimizer_name = self.optimizer.__class__.__name__\n",
    "        optimizer_params = ', '.join([f\"{k}={v}\" for k, v in self.optimizer.defaults.items() if k in [\"lr\", \"betas\", \"weight_decay\", \"fused\"]])\n",
    "        optimizer_display = f\"{optimizer_name}({optimizer_params})\"\n",
    "        table.add_row([\"Optimizer\", optimizer_display])\n",
    "        # Parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        te_params = self.model.wte.weight.numel()\n",
    "        table.add_row([\"Total Parameters (Tr+TE)\", f\"{total_params:,} ({total_params-te_params:,}+{te_params:,})\"])\n",
    "\n",
    "        table.add_row([\"Loss Function\", self.loss_fn.__name__ if hasattr(self.loss_fn, '__name__') else str(self.loss_fn)])\n",
    "        table.add_row([\"Batch Shape\", f\"{self.train_loader.batch_size}x{self.train_loader.dataset[0].shape[-1]-1}\"])\n",
    "        table.add_row([\"Device\", self.device])\n",
    "        table.add_row([\"Max Tokens\", f\"{self.total_tokens:,}\"])\n",
    "        table.add_row([\"Log Interval Tokens\", f\"{self.log_interval_tokens:,}\"])\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    tokenizer: Tokenizer\n",
    "    prompts: list[str]\n",
    "    T: float = 0.9\n",
    "    max_seq_len: int = 128\n",
    "    top_k: int = 10\n",
    "    n_rep: int = 3\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model, optimizer, train_loader, valid_loader,\n",
    "    total_tokens=100_000, log_interval_tokens=50_000,\n",
    "    generation=GenerationConfig(tokenizer=tokenizer, prompts=[\"In last\", \"One day\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "loader = DataLoader(TensorDataset(torch.arange(4)), batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "iterr = cycle(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=100, desc=\"Training\", unit='t') as pbar:\n",
    "    for i in range(10):\n",
    "        time.sleep(0.1)\n",
    "        pbar.set_postfix({'loss': torch.randn(1).item()})\n",
    "        pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Create a table\n",
    "table = PrettyTable()\n",
    "\n",
    "# Add columns\n",
    "table.field_names = [\"Name\", \"Position\", \"Age\"]\n",
    "\n",
    "# Add rows\n",
    "table.add_row([\"Alice\", \"Manager\", 35])\n",
    "table.add_row([\"Bob\", \"Data Analyst\", 29])\n",
    "table.add_row([\"Charlie\", \"Engineer\", 32])\n",
    "\n",
    "# Print the table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "\n",
    "prompt = 'In last'\n",
    "tokens = tokenizer.encode(prompt).ids\n",
    "tokens = torch.tensor(tokens, dtype=torch.int, device=device).unsqueeze(0)\n",
    "print(tokens)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(max_seq_len):\n",
    "        logits = model(tokens)\n",
    "        scores = logits[0, [-1]].softmax(dim=-1)\n",
    "        idx = scores.argmax(keepdims=True)\n",
    "        tokens = torch.cat((tokens, idx), dim=-1)\n",
    "\n",
    "tokenizer.decode(tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° `temperature` in `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw logits (e.g., output of a neural network before softmax)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# List of different temperatures to test\n",
    "temperatures = [0.05, 0.5, 1.0, 5.0]\n",
    "\n",
    "# Plotting setup\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    # Apply softmax with temperature\n",
    "    probs = torch.softmax(logits / T, dim=0)\n",
    "\n",
    "    # Print the probabilities\n",
    "    print(f\"Temperature = {T:.2f} ‚Üí Probabilities: {probs.numpy()}\")\n",
    "\n",
    "    # Plot the distribution\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.bar(range(len(probs)), probs.numpy(), color='skyblue')\n",
    "    plt.title(f\"Temperature = {T}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks([0, 1, 2], ['Class 1', 'Class 2', 'Class 3'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Effect of Temperature on Softmax Distribution\", fontsize=14, y=1.03);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'In last'\n",
    "tokens = tokenizer.encode(prompt).ids\n",
    "tokens = torch.tensor(tokens, dtype=torch.int, device=device).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(range(logits.shape[-1]), logits[0, -1].cpu(), width=1, edgecolor='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "T = 0.5\n",
    "T = 5.0\n",
    "probs = torch.softmax(logits[0, -1]/T, dim=0)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.bar(range(probs.shape[-1]), probs.cpu(), width=1, edgecolor='none')\n",
    "plt.ylim(0, 0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° `argmax` ‚û° `top-k` & `multinomial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5.\n",
    "probs = torch.softmax(logits[0, -1]/T, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.argmax(probs)\n",
    "tokenizer.decode([idx.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_probs, topk_indices = torch.topk(probs, k=10)\n",
    "print(topk_probs)\n",
    "print(topk_indices)\n",
    "\n",
    "tokenizer.decode(topk_indices.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.multinomial(topk_probs, 1)\n",
    "tokenizer.decode(topk_indices[idx].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.multinomial(probs, 1)\n",
    "tokenizer.decode(idx.tolist()), probs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.multinomial(topk_probs, 1)\n",
    "idx, torch.gather(topk_indices, 0, idx) # == topk_indices[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü° Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'In last',\n",
    "    'Once upon',\n",
    "    'Once upon a time',\n",
    "    'One day, a little boy named TimTommy was a smart 3 year old, much smarter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "n_rep = 5\n",
    "max_seq_len = 128\n",
    "T = 0.9\n",
    "top_k = 5\n",
    "\n",
    "inputs = torch.tensor(tokenizer.encode(prompts[item]).ids, dtype=torch.int, device=device) # T\n",
    "inputs = inputs.unsqueeze(0) # BxT\n",
    "inputs = inputs.repeat(n_rep, 1)\n",
    "print(inputs)\n",
    "print(100*'-')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(max_seq_len): # while\n",
    "        logits = model(inputs)\n",
    "        probs = torch.softmax(logits[:, -1, :]/T, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "        ids = torch.multinomial(topk_probs, 1)\n",
    "        ids = torch.gather(topk_indices, -1, ids)\n",
    "        inputs = torch.cat((inputs, ids), dim=-1) # Bx(T+1)\n",
    "\n",
    "generated_text = tokenizer.decode_batch(inputs.tolist())\n",
    "for text in generated_text:\n",
    "    pprint(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, n_rep=5, max_seq_len=128, T=0.9, top_k=10, device='cuda', seed=42):\n",
    "    # Tokenize the prompt and convert it to a tensor on the specified device (e.g., GPU)\n",
    "    inputs = torch.tensor(tokenizer.encode(prompt).ids, dtype=torch.int, device=device)  # Shape: [T]\n",
    "\n",
    "    # Repeat the input prompt n_rep times to generate multiple sequences in parallel\n",
    "    inputs = inputs.unsqueeze(0).repeat(n_rep, 1)  # Shape: [B, T] where B = n_rep\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize a random number generator for sampling\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(seed)\n",
    "\n",
    "    # Disable gradient calculation for faster inference\n",
    "    with torch.no_grad():\n",
    "        # Continue generating tokens until reaching the maximum sequence length\n",
    "        while inputs.shape[-1] < max_seq_len:\n",
    "            # Forward pass: get logits from the model\n",
    "            logits = model(inputs)  # Shape: [B, T, vocab_size]\n",
    "\n",
    "            # Apply temperature scaling and softmax to get probabilities for the next token\n",
    "            probs = torch.softmax(logits[:, -1, :] / T, dim=-1)  # Shape: [B, vocab_size]\n",
    "\n",
    "            # Select the top_k tokens with the highest probabilities\n",
    "            topk_probs, topk_indices = torch.topk(probs, k=top_k, dim=-1)  # Shape: [B, top_k]\n",
    "\n",
    "            # Sample one token from the top_k candidates based on their probabilities\n",
    "            ids = torch.multinomial(topk_probs, 1, generator=sample_rng)  # Shape: [B, 1]\n",
    "\n",
    "            # Map the sampled indices back to the original token IDs\n",
    "            ids = torch.gather(topk_indices, -1, ids)  # Shape: [B, 1]\n",
    "\n",
    "            # Append the sampled tokens to the input sequence\n",
    "            inputs = torch.cat((inputs, ids), dim=-1)  # Shape: [B, T+1]\n",
    "\n",
    "    # Decode the generated sequences back into text\n",
    "    generated_text = tokenizer.decode_batch(inputs.tolist())\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat_style(prompt, generated_text, tokenizer, delay=0.03):\n",
    "    \"\"\"\n",
    "    Display generated text in a token-by-token ChatGPT-like style:\n",
    "    - prompt in green\n",
    "    - generated continuation in blue\n",
    "    \"\"\"\n",
    "    for i, full_text in enumerate(generated_text):\n",
    "        print(colored(f\"\\n[Sample {i+1}]\", \"yellow\"))\n",
    "        input_ids = tokenizer.encode(prompt).ids\n",
    "        full_ids = tokenizer.encode(full_text).ids\n",
    "\n",
    "        # Split into prompt tokens and continuation\n",
    "        prompt_tokens = full_ids[:len(input_ids)]\n",
    "        continuation_tokens = full_ids[len(input_ids):]\n",
    "\n",
    "        # Decode tokens separately\n",
    "        prompt_text = tokenizer.decode(prompt_tokens)\n",
    "        cont_tokens_text = [tokenizer.decode([tid]) for tid in continuation_tokens]\n",
    "\n",
    "        # Print prompt in green\n",
    "        sys.stdout.write(colored(prompt_text, 'green'))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print continuation token-by-token in blue\n",
    "        for token in cont_tokens_text:\n",
    "            sys.stdout.write(colored(token, 'cyan'))\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(delay)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In last\"\n",
    "generated = generate(model, tokenizer, prompt=prompt, n_rep=2, max_seq_len=128)\n",
    "display_chat_style(prompt=prompt, generated_text=generated, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Training Process „ÄΩÔ∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "seq_len = 128\n",
    "train_set = TinyStoriesDataset(train_token_ids, seq_len)\n",
    "valid_set = TinyStoriesDataset(valid_token_ids, seq_len)\n",
    "\n",
    "batch_size = 192\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)#, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)#, num_workers=4)\n",
    "\n",
    "print(f\"üìä Number of Batches\")\n",
    "print(f\"üîπ Train: {len(train_loader):,} batches\")\n",
    "print(f\"üîπ Valid: {len(valid_loader):,} batches\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
