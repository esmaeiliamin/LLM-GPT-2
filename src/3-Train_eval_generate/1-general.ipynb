{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#FFE15D'><b>ðŸ’Ž Train, Evaluate, and Generate Functions (General)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Environment Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Change the font size of the output cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "shell = get_ipython()\n",
    "\n",
    "def adjust_font_size():\n",
    "  display(HTML('''<style>\n",
    "    body {\n",
    "      font-size: 24px;\n",
    "    }\n",
    "  '''))\n",
    "\n",
    "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
    "  shell.events.register('pre_execute', adjust_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmetrics.aggregation import MeanMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking function\n",
    "def calculate_time(model, x, num_runs=10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens from pytorch file\n",
    "train_token_ids = torch.load('tokenized-train-samples_vocab-10k.pt')\n",
    "valid_token_ids = torch.load('tokenized-valid-samples_vocab-10k.pt')\n",
    "\n",
    "print(\"ðŸ“Š Number of Tokens\")\n",
    "print(f\"ðŸ”¹ Train: {len(train_token_ids):,} tokens\")\n",
    "print(f\"ðŸ”¹ Valid: {len(valid_token_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = prepare_data(data, seq_len+1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample.long()#[:-1], sample[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”´ **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŸ  Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # QKV linear\n",
    "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
    "        # Scaled Dot Product Attention using pytorch\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # Reshape and final projection\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
