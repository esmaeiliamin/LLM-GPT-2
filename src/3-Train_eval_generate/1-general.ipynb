{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#FFE15D'><b>üíé Train, Evaluate, and Generate Functions (General)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Environment Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Change the font size of the output cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "shell = get_ipython()\n",
    "\n",
    "def adjust_font_size():\n",
    "  display(HTML('''<style>\n",
    "    body {\n",
    "      font-size: 24px;\n",
    "    }\n",
    "  '''))\n",
    "\n",
    "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
    "  shell.events.register('pre_execute', adjust_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmetrics.aggregation import MeanMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking function\n",
    "def calculate_time(model, x, num_runs=10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens from pytorch file\n",
    "train_token_ids = torch.load('tokenized-train-samples_vocab-10k.pt')\n",
    "valid_token_ids = torch.load('tokenized-valid-samples_vocab-10k.pt')\n",
    "\n",
    "print(\"üìä Number of Tokens\")\n",
    "print(f\"üîπ Train: {len(train_token_ids):,} tokens\")\n",
    "print(f\"üîπ Valid: {len(valid_token_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = prepare_data(data, seq_len+1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample.long()#[:-1], sample[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # QKV linear\n",
    "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
    "        # Scaled Dot Product Attention using pytorch\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # Reshape and final projection\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Feed Forward (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.f_expnd = config.f_expnd\n",
    "\n",
    "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
    "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
    "        self.down_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.up_proj(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        # Multi Head Attention\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        # Feed Forward Neural Network\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # Token embedding\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.n_embd) # Position embedding\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]) # Decoders\n",
    "        self.lnf = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Classifier\n",
    "        self.lm_head.weight = self.wte.weight # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'residual'):\n",
    "                std *= (2*self.config.n_layer)**-0.5\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        # Token Embedding + Position Embedding\n",
    "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
    "        # Decoders\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x)\n",
    "        # Classifier\n",
    "        x = self.lnf(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257 # number of tokens\n",
    "    max_seq_len: int = 1024 # max sequence length\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    f_expnd: int = 4 # expansion factor in mlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Functions ‚öôÔ∏è**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Train ‚û∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, epoch=None):\n",
    "    model.train()\n",
    "    loss_train = MeanMetric()\n",
    "\n",
    "    with tqdm(train_loader, unit='batch') as tepoch:\n",
    "        for inputs in tepoch:\n",
    "            if epoch is not None:\n",
    "                tepoch.set_description(f'Epoch {epoch}')\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs[:, :-1])\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm=1.)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train.update(loss.item(), inputs.shape[0])\n",
    "\n",
    "            tepoch.set_postfix(loss=loss_train.compute().item())\n",
    "\n",
    "    return loss_train.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    loss_eval = MeanMetric()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            logits = model(inputs[:, :-1])\n",
    "\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "            loss_eval.update(loss.item(), inputs.shape[0])\n",
    "\n",
    "    return loss_eval.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "        model, tokenizer, xgen, max_length, temperature=1.0, top_k=50, num_return_sequences=1, device='cpu', seed=42):\n",
    "    \"\"\"\n",
    "    Generates text using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: Tokenizer for decoding generated text.\n",
    "        xgen: Initial token sequence (Tensor of shape (B, T)).\n",
    "        max_length: Maximum length of generated text.\n",
    "        temperature: Sampling temperature.\n",
    "        top_k: Number of top candidates to sample from.\n",
    "        num_return_sequences: Number of generated sequences.\n",
    "        device: Device to run the model on.\n",
    "        seed: Random seed for reproducibility.\n",
    "        epoch: Current epoch number (for logging).\n",
    "    \"\"\"\n",
    "    # Initialize a random number generator for sampling\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(seed)\n",
    "\n",
    "    generated_texts = []\n",
    "\n",
    "    with torch.no_grad():  # Move outside the loop to optimize performance\n",
    "        while xgen.size(1) < max_length:\n",
    "            logits = model(xgen)  # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :]  # Take the logits at the last position (B, vocab_size)\n",
    "            probs = F.softmax(logits / temperature, dim=-1)  # Get the probabilities\n",
    "\n",
    "            # Perform top-k sampling (default k=50 in Hugging Face pipeline)\n",
    "            topk_probs, topk_indices = torch.topk(probs, top_k, dim=-1)  # (B, top_k)\n",
    "\n",
    "            # Select a token from the top-k probabilities\n",
    "            # Note: multinomial does not require the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng)  # (B, 1)\n",
    "\n",
    "            # Gather the corresponding indices from the top-k indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
    "\n",
    "            # Append the selected token to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "    # Decode and return generated sequences\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = xgen[i, :max_length].tolist()\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        generated_texts.append(decoded_text)\n",
    "\n",
    "    return generated_texts  # Return generated sequences as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Training Process „ÄΩÔ∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "seq_len = 128\n",
    "train_set = TinyStoriesDataset(train_token_ids, seq_len)\n",
    "valid_set = TinyStoriesDataset(valid_token_ids, seq_len)\n",
    "\n",
    "batch_size = 192\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)#, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, pin_memory=True)#, num_workers=4)\n",
    "\n",
    "print(f\"üìä Number of Batches\")\n",
    "print(f\"üîπ Train: {len(train_loader):,} batches\")\n",
    "print(f\"üîπ Valid: {len(valid_loader):,} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    GPTConfig(\n",
    "        max_seq_len=256,\n",
    "        vocab_size=10_000,\n",
    "        n_embd=128,\n",
    "        n_layer=8,\n",
    "        n_head=16\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "print(model)\n",
    "num_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 6e-4\n",
    "weight_decay = 0.1\n",
    "use_fused = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=weight_decay,\n",
    "    fused=use_fused\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_hist = []\n",
    "loss_valid_hist = []\n",
    "\n",
    "metric_train_hist = []\n",
    "metric_valid_hist = []\n",
    "\n",
    "best_loss_valid = torch.inf\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'In last night',\n",
    "    'Once upon',\n",
    "    'Once upon a time',\n",
    "    'One day, a little boy named TimTommy was a smart 3 year old, much smarter']\n",
    "\n",
    "num_return_sequences = 3\n",
    "sent_test = [\n",
    "    torch.tensor(tokens.ids, dtype=torch.long).unsqueeze(0).repeat(num_return_sequences, 1).to(device)\n",
    "    for tokens in tokenizer.encode_batch(prompts)]\n",
    "sent_test\n",
    "\n",
    "temperature = 0.9\n",
    "max_length = 32\n",
    "top_k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(0, num_epochs+1):\n",
    "    # Train\n",
    "    loss_train = train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "\n",
    "    # Validation\n",
    "    loss_valid = evaluate(model, valid_loader)\n",
    "    print(f'Valid: Loss = {loss_valid:.4}')\n",
    "\n",
    "    loss_train_hist.append(loss_train)\n",
    "    loss_valid_hist.append(loss_valid)\n",
    "\n",
    "    if loss_valid < best_loss_valid:\n",
    "        torch.save(model, f'best-model.pt')\n",
    "        best_loss_valid = loss_valid\n",
    "        print(f'Model Saved!')\n",
    "    print(150*'.')\n",
    "\n",
    "    # Generate\n",
    "    if epoch % 3 == 0 or epoch == num_epochs:\n",
    "        model.eval()\n",
    "        for xgen in sent_test:\n",
    "            xgen = generate_text(model, tokenizer, xgen, max_length, temperature, 50, 3, device)\n",
    "            for i, text in enumerate(xgen):\n",
    "                print(f\"Epoch {epoch} Sample {i}: {text}\")\n",
    "            print(150*'.')\n",
    "\n",
    "    epoch_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(range(len(loss_train_hist)), loss_train_hist, 'r-', label='Train')\n",
    "plt.plot(range(len(loss_valid_hist)), loss_valid_hist, 'b-', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
