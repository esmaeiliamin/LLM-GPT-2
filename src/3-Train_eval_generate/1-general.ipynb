{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='#FFE15D'><b>üíé Train, Evaluate, and Generate Functions (General)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Environment Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Change the font size of the output cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "shell = get_ipython()\n",
    "\n",
    "def adjust_font_size():\n",
    "  display(HTML('''<style>\n",
    "    body {\n",
    "      font-size: 24px;\n",
    "    }\n",
    "  '''))\n",
    "\n",
    "if adjust_font_size not in shell.events.callbacks['pre_execute']:\n",
    "  shell.events.register('pre_execute', adjust_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchmetrics.aggregation import MeanMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens, seq_len):\n",
    "    # Trim tokens so that total length is divisible by seq_len\n",
    "    n_tokens = (tokens.shape[0] // seq_len) * seq_len\n",
    "    tokens = tokens[:n_tokens]\n",
    "    # Reshape to 2D tensor\n",
    "    return tokens.view(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking function\n",
    "def calculate_time(model, x, num_runs=10):\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"bpe-tokenizer_tinystories.json\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokens from pytorch file\n",
    "train_token_ids = torch.load('tokenized-train-samples_vocab-10k.pt')\n",
    "valid_token_ids = torch.load('tokenized-valid-samples_vocab-10k.pt')\n",
    "\n",
    "print(\"üìä Number of Tokens\")\n",
    "print(f\"üîπ Train: {len(train_token_ids):,} tokens\")\n",
    "print(f\"üîπ Valid: {len(valid_token_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = prepare_data(data, seq_len+1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample.long()#[:-1], sample[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_size = self.n_embd // self.n_head\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.n_embd, 3*self.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # QKV linear\n",
    "        q, k, v = self.qkv_proj(x).view(B, T, 3*self.n_head, self.head_size).transpose(1, 2).chunk(3, dim=-3)\n",
    "        # Scaled Dot Product Attention using pytorch\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # Reshape and final projection\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Feed Forward (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.f_expnd = config.f_expnd\n",
    "\n",
    "        self.up_proj = nn.Linear(self.n_embd, int(self.f_expnd*self.n_embd), bias=False)\n",
    "        self.down_proj = nn.Linear(int(self.f_expnd*self.n_embd), self.n_embd, bias=False)\n",
    "        self.down_proj.residual = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.gelu(self.up_proj(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        # Multi Head Attention\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        # Feed Forward Neural Network\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # Token embedding\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.n_embd) # Position embedding\n",
    "        self.decoders = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]) # Decoders\n",
    "        self.lnf = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Classifier\n",
    "        self.lm_head.weight = self.wte.weight # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'residual'):\n",
    "                std *= (2*self.config.n_layer)**-0.5\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        # Token Embedding + Position Embedding\n",
    "        x = self.wte(idx) + self.wpe(torch.arange(T, device=device))\n",
    "        # Decoders\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x)\n",
    "        # Classifier\n",
    "        x = self.lnf(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257 # number of tokens\n",
    "    max_seq_len: int = 1024 # max sequence length\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    f_expnd: int = 4 # expansion factor in mlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¥ **Functions ‚öôÔ∏è**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü† Train ‚û∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, epoch=None):\n",
    "    model.train()\n",
    "    loss_train = MeanMetric()\n",
    "\n",
    "    with tqdm(train_loader, unit='batch') as tepoch:\n",
    "        for inputs in tepoch:\n",
    "            if epoch is not None:\n",
    "                tepoch.set_description(f'Epoch {epoch}')\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            logits = model(inputs[:, :-1])\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.shape[-1]), inputs[:, 1:].flatten())\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad.clip_grad_norm_(model.parameters(), max_norm=1.)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train.update(loss.item(), inputs.shape[0])\n",
    "\n",
    "            tepoch.set_postfix(loss=loss_train.compute().item())\n",
    "\n",
    "    return loss_train.compute().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
